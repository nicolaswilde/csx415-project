---
title: "project-performance"
author: "Chongxi Wang"
date: "May 29, 2018"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
```

There are two kinds of model used in this project. Regression model is used in salary prediction and classification model is used in game result prediction.

Linear model, CSRF model, random forest model are cosidered in game result prediction.

Linear model, CSRF model, random forest model, and adaboost model are considered in game result prediction.

## Salary Prediction Model Performance

Prepare data.

```{r}
# load 2017/18 salary
salary <- read.csv("..\\data\\PlayerSalary.csv", stringsAsFactors=F)
salary_1718 <- salary[, c("Player", "Conversion1718")]
print(salary_1718)

# load 2017/18 regular season player stats per game
player_stats_per_game_1718 <- read.csv("..\\data\\PlayerStatsPerGame1718.csv", stringsAsFactors=F)
player_stats_per_game_1718 <- player_stats_per_game_1718[, c("Player", "Age", "G", "GS", "MP", "FG", "FGA", "FGP", "X3P", "X3PA", "X3PP", "X2P", "X2PA", "X2PP", "eFGP", "FT", "FTA", "FTP", "ORB", "DRB", "TRB", "AST", "STL", "BLK", "TOV", "PF", "PTS")]
print(player_stats_per_game_1718)

# merge salary and performance
salary_data <- merge(salary_1718, player_stats_per_game_1718, by="Player")
print(salary_data)
```

Data Partition.

```{r}
salary_index <- createDataPartition(salary_data$Conversion1718, p=0.75, list=FALSE)
salary_train <- salary_data[salary_index,]
salary_test <- salary_data[-salary_index,]
print(nrow(salary_train))
print(nrow(salary_test))
```

### Model 1: Linear Model

Train model.

```{r}
salary_model_lm <- train(Conversion1718~Age+G+GS+MP+FG+FGA+FGP+X3P+X3PA+X3PP+X2P+X2PA+X2PP+eFGP+FT+FTA+FTP+ORB+DRB+TRB+AST+STL+BLK+TOV+PF+PTS, data=salary_train, method="glm")
```

Test model.

```{r}
salary_pred_lm <- predict(salary_model_lm, salary_test)
```

Model performance.

```{r}
postResample(pred=salary_pred_lm, obs=salary_test$Conversion1718)
```

### Model 2: CART Model

Train model.

```{r}
salary_model_cart <- train(Conversion1718~Age+G+GS+MP+FG+FGA+FGP+X3P+X3PA+X3PP+X2P+X2PA+X2PP+eFGP+FT+FTA+FTP+ORB+DRB+TRB+AST+STL+BLK+TOV+PF+PTS, data=salary_train, method="rpart")
```

Test model.

```{r}
salary_pred_cart <- predict(salary_model_cart, salary_test)
```

Model performance.

```{r}
postResample(pred=salary_pred_cart, obs=salary_test$Conversion1718)
```

### Model 3:Random Forest Model

Train model.

```{r}
salary_model_rf <- train(Conversion1718~Age+G+GS+MP+FG+FGA+FGP+X3P+X3PA+X3PP+X2P+X2PA+X2PP+eFGP+FT+FTA+FTP+ORB+DRB+TRB+AST+STL+BLK+TOV+PF+PTS, data=salary_train, method="rf")
```

Test model.

```{r}
salary_pred_rf <- predict(salary_model_rf, salary_test)
```

Model performance.

```{r}
postResample(pred=salary_pred_rf, obs=salary_test$Conversion1718)
```

### Conclusion

Rsquared:

- Linear model: 0.4989

- CART model: 0.3379

- Random forest model: 0.5931

The random forest model has the best performance.

## Game Prediction Model Performance

Prepare data.

```{r}
# load 3 chars team id
team_list <- read.csv("..\\data\\TeamList.csv", stringsAsFactors=F)
print(team_list)

# load 2017/18 regular season team stats per game
team_stats_per_game_1718 <- read.csv("..\\data\\TeamStatsPerGame1718.csv", stringsAsFactors=F)
team_stats_per_game_1718 <- team_stats_per_game_1718[, c("Team", "FG", "FGA", "FGP", "X3P", "X3PA", "X3PP", "X2P", "X2PA", "X2PP", "FT", "FTA", "FTP", "ORB", "DRB", "TRB", "AST", "STL", "BLK", "TOV", "PF", "PTS")]
print(team_stats_per_game_1718)

# biuld dataset according to team game log
# format: own avg stats, opp avg stats, W/L
game_data = data.frame()
for (i in 1:30) {
    team_id <- team_list[,1][i]
    team_data <- subset(team_stats_per_game_1718, Team==team_id, select=c("FG", "FGA", "FGP", "X3P", "X3PA", "X3PP", "X2P", "X2PA", "X2PP", "FT", "FTA", "FTP", "ORB", "DRB", "TRB", "AST", "STL", "BLK", "TOV", "PF", "PTS"))
    team_log <- read.csv(paste("..\\data\\TeamGameLog1718\\", team_id, ".csv", sep=""), stringsAsFactors=F)
    for (j in 1:82) {
        opp_team_id <- team_log[,"Opp"][j]
        opp_team_data <- subset(team_stats_per_game_1718, Team==opp_team_id, select=c("FG", "FGA", "FGP", "X3P", "X3PA", "X3PP", "X2P", "X2PA", "X2PP", "FT", "FTA", "FTP", "ORB", "DRB", "TRB", "AST", "STL", "BLK", "TOV", "PF", "PTS"))
        game_result <- team_log[,"W.L"][j]
        new_game_data <- data.frame(team_data[1:21], opp_team_data[1:21], game_result)
        game_data <- rbind(game_data, new_game_data)
    }
}
print(game_data)
```

Data partition.

```{r}
inTrain <- createDataPartition(y=game_data$game_result, p=0.75, list=FALSE)
training <- game_data[inTrain,]
testing <- game_data[-inTrain,]
print(nrow(training))
print(nrow(testing))
```

### Model 1: linear model.

Train model.

```{r}
game_model_lm <- train(game_result~., data=training, method="glm")
```

Test model.

```{r}
game_pred_lm <- predict(game_model_lm, newdata=testing)
```

Model performance.

```{r}
confusionMatrix(game_pred_lm, testing$game_result)
```

### Model 2: CART model.

Train model.

```{r}
game_model_cart <- train(game_result~., data=training, method="rpart")
```

Test model.

```{r}
game_pred_cart <- predict(game_model_cart, newdata=testing)
```

Model performance.

```{r}
confusionMatrix(game_pred_cart, testing$game_result)
```

### Model 3: Random forest model.

Train model.

```{r}
game_model_rf <- train(game_result~., data=training, method="rf")
```

Test model.

```{r}
game_pred_rf <- predict(game_model_rf, newdata=testing)
```

Model performance.

```{r}
confusionMatrix(game_pred_rf, testing$game_result)
```

### Model 4: Adaboost model.

Train model.

```{r}
game_model_ada <- train(game_result~., data=training, method="adaboost")
```

Test model.

```{r}
game_pred_ada <- predict(game_model_ada, newdata=testing)
```

Model performance.

```{r}
confusionMatrix(game_pred_ada, testing$game_result)
```

### Conclusion

Accuracy:

- Generalized Linear Model: 0.6433

- CART Model: 0.5912

- Random Forest Model: 0.601

- Adaboost Model: 0.5749

Kappa:

- Generalized Linear Model: 0.2866

- CART Model: 0.1824

- Random Forest Model: 0.202

- Adaboost Model: 0.1498


Surprisingly, Generalized Linear Model is the best model.

Additionally, it is acceptable that the accuracy is about only 60% because different games between the same teams often have different result.